---
layout: post
title: "Performance boost on ROCm Backend with new Attention Design"
author: "AMD and Embedded LLM"
image: /assets/figures/ptpc/PTPC-tumbnail.png
math: true
---

## Introduction

Attention backend is one of the most compilicated module in vLLM, which is deeply bound to the hardware platform and needs to support different configs used on Model side. Even in the same hardware platform, there are some different attention backends to support different user scenarios delivering different performance. In this post, we will introduce the attention backends on AMD's ROCm platform. We will elaborate the priciples of the attention backend implementation, especially the `ROCM_AITER_FA`, along with the corresponding benchmark performance. This post aims to provide a reference for AMD users to select attention backends.  

Generally, the attention modules of common large language models can be categorized into two main types: **Multi-Head Attention (MHA)** and **Multi-Latent Attention (MLA)**. Qwen/Llama are using MHA module, while Deepseek/Kimi are using MLA module. In vLLM, both the two kinds of attention modules are supported on ROCm backend. Even more, there are four multi-head attention (MHA) backends and three Multi-Latent Attention (MLA) backends supported on AMD's platform, which can be summarized as following.

| Category | Backend | How to enable (New) | How to enable (Older Interface) | Sink Attention Support |
|:----------|:----------------|:--------------|:--------------|:--------------| 
| MHA | TRITON_ATTN | `--attention-backend TRITON_ATTN` | export VLLM_ATTENTION_BACKEND=TRITON_ATTN | Yes |
| MHA | ROCM_AITER_UNIFIED_ATTN | `--attention-backend ROCM_AITER_UNIFIED_ATTN` | export VLLM_ATTENTION_BACKEND=ROCM_AITER_UNIFIED_ATTN | Yes |
| MHA | ROCM_ATTN | `--attention-backend ROCM_ATTN` |  export VLLM_ATTENTION_BACKEND=ROCM_ATTN | No |
| MHA | ROCM_AITER_FA | `--attention-backend ROCM_AITER_FA` |  export VLLM_ATTENTION_BACKEND=ROCM_AITER_FA | No |
| MLA | TRITON_MLA | `--attention-backend TRITON_MLA` |  export VLLM_ATTENTION_BACKEND=TRITON_MLA | No |
| MLA | ROCM_AITER_MLA | `--attention-backend ROCM_AITER_MLA` |  export VLLM_ATTENTION_BACKEND=ROCM_AITER_MLA | No |
| MLA | ROCM_AITER_TRITON_MLA | `--attention-backend ROCM_AITER_TRITON_MLA` |  export VLLM_ATTENTION_BACKEND=ROCM_AITER_TRITON_MLA | No |

**NOTE:**:

- If you are using older vLLM version, there might not have the `--attention-backend` interface, use `VLLM_ATTENTION_BACKEND` instead.

- [AITER](https://github.com/ROCm/aiter) is AMD‚Äôs centralized repository that support various of high performance AI operators for AI workloads acceleration. Attention backends that are using AITER has `_AITER_` in the backend name.

## Multi-Head Attention Backend

### The Unified Attention backend

The principle of the unified attention backend is to first write the key and value of the scheduled tokens generated by QKV gemm into the KV cache buffer, then read all of the KV cache (including the scheduled token KV cache and the context KV cache) to compute the attention output by invoking the attention kernel only once, regardless of whether chunked prefill occurs.

vLLM ROCm backend now supports two unified attention implementations: one is [Triton unified attention](https://github.com/vllm-project/vllm/blob/main/vllm/v1/attention/backends/triton_attn.py#L265), which is developed by vllm community, and the other is [AITER unified attention](https://github.com/vllm-project/vllm/blob/main/vllm/v1/attention/backends/rocm_aiter_unified_attn.py). Both the implementations follows the same principle but use the different kernels‚Äîthe former invokes the default [vLLM Triton attention kernel](https://github.com/vllm-project/vllm/blob/main/vllm/attention/ops/triton_unified_attention.py#L735), while the latter calls the AITER Triton attention kernel.

Below is the pseudo-code illustrating how unified attention works under the chunked prefill scenario:
```python
# Step 1: Schedule a single request, whose prompt length is 20k
# vllm server is launched by enabling chunked prefill and max num batched tokens is specified as 16k
# so the first step is only 16384 tokens are scheduled for this request
# Stage 1: Save the Key/Value of new tokens of this new request into KV-Cache
reshape_and_cache(new_key, new_value, ...)  # kv_len=16384

# Stage 2: Read the KV-Cache for attention computation
unified_attention(new_query, KV-Cache, ...)  # kv_cache_len=16384

# Step 2: Schedule the remaining tokens of this request, the remaining tokens is 3616
# Stage 1: Save the Key/Value of remaining tokens into KV-Cache
reshape_and_cache(new_key, new_value, ...)  # kv_len=3616

# Stage 2: Read the KV-Cache (new Key/Value and history Key/Value) for attention computation
unified_attention(new_query, KV-Cache, ...)  # kv_cache_len=20000
```

#### TRITON_ATTN - The Community-Driven Unified Backend

**Design Overview**

TRITON_ATTN (`vllm/v1/attention/backends/triton_attn.py`) represents vLLM's community-developed cross-platform unified attention backend. This implementation uses a sophisticated 2D/3D adaptive kernel approach to handle various workload scenarios efficiently. The backend stores KV cache in the layout `(num_blocks, 2, block_size, num_kv_heads, head_size)`, which is optimized for coalesced memory access patterns common in GPU architectures.

**Adaptive Kernel Selection**

One of the key innovations in TRITON_ATTN is its dynamic kernel selection strategy. The backend chooses between two kernel variants based on batch characteristics:

- **2D Kernel**: Selected when the number of sequences meets a minimum threshold. This kernel processes attention in a two-dimensional grid, optimized for larger batches where parallelism across sequences is beneficial.

- **3D Kernel**: Used for smaller batches or when more fine-grained parallelism is needed. To handle long contexts without running into memory constraints, the 3D kernel employs **parallel softmax segmentation** with 16 segments. This segmentation allows the kernel to process attention scores in chunks, maintaining numerical stability while keeping memory usage bounded.

**Technical Features**

The backend supports a comprehensive set of modern attention features:
- FP8 KV cache quantization (both e4m3 and e5m2 formats) for reduced memory footprint
- Sink tokens for efficient attention with special tokens
- Sliding window attention for models with local attention patterns
- Alibi slopes for positional bias
- Logits soft capping for stability

**Pseudo-code**:
```python
def triton_unified_attention():
    # Stage 1: Write new KV to cache with optional quantization
    reshape_and_cache(key, value, kv_cache, scale=fp8_scale)

    # Stage 2: Select kernel based on batch characteristics
    if num_sequences >= SEQ_THRESHOLD_3D:
        # 2D kernel for larger batches
        output = unified_attention_2d(
            query, kv_cache,
            block_tables,
            ...
        )
    else:
        # 3D kernel with parallel softmax segments
        output = unified_attention_3d(
            query, kv_cache,
            block_tables,
            num_softmax_segments=16,  # Chunked softmax for memory efficiency
            ...
        )

    return output
```

**Key Trade-offs**

The unified approach offers significant advantages in simplicity and flexibility. By using a single kernel for both prefill and decode, it simplifies the execution graph and reduces code complexity. The adaptive 2D/3D selection ensures good performance across various batch sizes. However, this generality comes with a trade-off: for workloads with extreme decode-to-prefill ratios, specialized kernels might offer better performance by exploiting the specific characteristics of each phase.

#### ROCM_AITER_UNIFIED_ATTN - AMD-Optimized Unified Approach

**Design Philosophy**

ROCM_AITER_UNIFIED_ATTN (`vllm/v1/attention/backends/rocm_aiter_unified_attn.py`) represents a thin wrapper over vLLM's ROCm attention implementation that leverages the external AITER library's optimized unified attention kernel. Rather than reimplementing attention from scratch, this backend integrates AMD's specialized AITER kernels, which are developed specifically for ROCm and tuned for AMD CDNA architecture GPUs like MI300 series.

**Integration Strategy**

The backend inherits the same KV cache layout as the standard ROCm backend: `(2, num_blocks, block_size, num_kv_heads, head_size)`. This layout places the key/value dimension first, which aligns well with certain memory access patterns on AMD hardware. The thin wrapper design means vLLM's codebase remains clean and maintainable while still benefiting from AMD's continuously evolving kernel optimizations.

**AMD-Specific Optimizations**

The AITER unified attention kernel incorporates several AMD-specific optimizations:
- Hardware-aware memory access patterns
- Tuned for AMD's wavefront size and LDS (Local Data Share) usage patterns
- Supports sink attention, thus it is compatible with GPT-OSS.

**Pseudo-code**:
```python
...

from aiter.ops.triton.unified_attention import unified_attention
...

def rocm_aiter_unified_attention():
    # Stage 1: Cache new KV with FP8 quantization support
    ops.reshape_and_cache_flash(
        key,
        value,
        key_cache,
        value_cache,
        attn_metadata.slot_mapping,
        self.kv_cache_dtype,
        layer._k_scale,
        layer._v_scale,
    )

    # Stage 2: Single AITER kernel call
    output = unified_attention(
        q=query[:num_actual_tokens],
        k=key_cache,
        v=value_cache,
        out=output[:num_actual_tokens],
        cu_seqlens_q=cu_seqlens_q,
        max_seqlen_q=max_seqlen_q,
        ...
    )

    return output
```

**When to Choose ROCM_AITER_UNIFIED_ATTN**

This backend is an excellent choice when:
- Running on GPT-OSS on AMD MI300 series GPUs
- The AITER library is available and well-tuned for your specific GPU generation
- Workload characteristics don't require specialized decode/prefill/extend paths

### The ROCM_ATTN Backend - Partition-Based Dual-Kernel Approach

While unified attention backends use a "write once, read once" approach, the ROCM_ATTN backend takes a fundamentally different strategy: divide and conquer through partitioning.

**Design Overview**

ROCM_ATTN (`vllm/v1/attention/backends/rocm_attn.py`) implements a hybrid dual-kernel architecture specifically optimized for AMD ROCm platforms. The core principle is to use specialized kernels for different phases of attention computation‚ÄîTriton-based flash attention for prefill and ROCm's custom PagedAttention kernel for decode. This specialization allows each kernel to be highly optimized for its specific workload characteristics.

The backend uses a KV cache layout of `(2, num_blocks, block_size, num_kv_heads, head_size)`. Note the leading dimension ordering: the key/value selector comes first, followed by block organization. This layout differs from TRITON_ATTN and is specifically chosen to align with ROCm PagedAttention kernel's memory access patterns on AMD hardware.

**Dual-Kernel Architecture**

The backend's design centers on recognizing that prefill and decode have fundamentally different computational characteristics. It utilizes a triton kernel `context_attention_fwd` to perform prefills. For decode (single-token generation), the backend switches to ROCm's specialized PagedAttention kernel `torch.ops._rocm_C.paged_attention` if the head sizes are (64, 128), block sizes are (16, 32), GQA ratios are (1-16), and context up to 131k. This kernel is optimized for the memory-bound characteristics of decode: accessing a long KV cache to compute attention for just one new query token. The PagedAttention kernel uses block tables for efficient, non-contiguous memory access, allowing it to handle paged KV cache layouts without expensive memory reorganization. If the condition is not met, the fallback kernel is a triton kernel `kernel_paged_attention_2d`.

**The Partition-Based Prefill Strategy**

Here's how partition-based prefill works in detail:

```python
def rocm_attn_forward():

    if max_query_len > 1:
        context_attention_fwd(
            q=query,
            k=key,
            v=value,
            o=output,
            ...
        )

    if use_rocm_custom_paged_attention:
        # Prefill path: Partition-based computation
        PARTITION_SIZE = 256

        max_num_partitions = (
            max_seq_len + _PARTITION_SIZE_ROCM - 1
        ) // _PARTITION_SIZE_ROCM
        assert _PARTITION_SIZE_ROCM % block_size == 0
        total_num_seq = block_table.shape[0]
        tmp_output = torch.empty(
            size=(total_num_seq, num_query_heads, max_num_partitions, head_size),
            dtype=query.dtype,
            device=output.device,
        )

        ops.paged_attention_rocm(
            output,
            exp_sums,
            max_logits,
            tmp_output,
            query,
            key_cache,
            value_cache,
            num_kv_heads,
            ...
        )

        return output
    else:
        kernel_paged_attention_2d(...)

        return output
```

**Advantages**:
- **Specialized Optimization**: Each kernel (Triton for prefill, PagedAttention for decode) is highly optimized for its specific workload
- **Predictable Memory Bounds**: Partition-based approach provides deterministic memory usage, preventing OOM errors
- **Decode Performance**: Dedicated PagedAttention kernel delivers excellent single-token generation performance
- **Mixed Workload Handling**: Can efficiently handle varying sequence lengths in the same batch

**When to Use ROCM_ATTN**

This backend is particularly well-suited for:
- **Mixed Workloads**: Scenarios with both long prefills and decode requests in the same batch
- **Memory-Constrained Deployments**: The partition-based approach provides guaranteed memory bounds
- **Decode-Heavy Workloads**: Applications where most requests are generating tokens (decode) rather than processing prompts
- **Production Deployments**: Where predictable memory usage and stability are more important than absolute peak performance


### The ROCM AITER FA backend 

This attention backend utilizes the AITER high-performance MHA kernel for computation, hence named as AITER Multi-head Attention backend. This backend can be employed by models with MHA/GQA structured attention modules during the prefill phase, such as Llama and Qwen serial models.

**Design Overview** 
![AITER FA Backend](/assets/figures/2025-12-16-rocm-attention-backend/AiterFaBackend.png)

The backend's performance leap stems from two fundamental design choices that re-think standard attention computation:

***Request Routing***: It dynamically categorizes and processes incoming requests into three distinct paths: Prefill, Decode, and Extend. This allows for specialized, optimized kernels to be applied to each specific workload type.

***Hardware-Optimized KV Cache Layout***: The backend abandons the standard KV cache layout in favor of a shuffled format designed for AITER's `pa_asm` (an assembly-optimized paged attention kernel). The new layouts are:

`k_cache: [num_blocks, num_heads, head_dim // x, block_size, x]`

`v_cache: [num_blocks, num_heads, block_size // x, head_dim, x]`
This reorganization ensures the `pa_asm` kernel experiences the most efficient memory access patterns possible, minimizing latency.

**Performance-Optimized Processing Paths**
Requests are reordered and processed in a decode:extend:prefill sequence, with each type taking a distinct, optimized path:

üîÅ **Decode Path (For Single Token Generation)**: Leverages the new shuffled KV cache layout directly. A custom `reshape_and_cache` kernel ensures the cache is always in the optimal format, allowing the backend to call pa_asm with zero layout conversion overhead. This is the source of the **15-20% decode throughput improvement**.

‚ú® **Prefill Path (For Processing New Prompts)**: Uses the highly optimized `flash_attn_varlen_func` directly, as the standard `[num_tokens, num_heads, head_dim]` layout is already ideal for this operation. Results are written in-place to avoid any extra memory copies.

üîÑ **Extend Path (For Context Extension)**: Presents a unique challenge. The shuffled layout is poor for long-sequence computation, but we don't want to lose its decode benefits. Our solution is a KV Cache Fetcher:

- New tokens are computed using `flash_attn_varlen_func`.

- The existing context is fetched in chunks, reordered into a prefill-friendly layout, and computed.

- `Attention outputs (attn_out)` and `log-sum-exp values (lse)` from new and cached tokens are merged to produce the final, correct output.
This chunked, buffered approach prevents out-of-memory (OOM) errors with extremely long sequences while maintaining high performance.

**Pseudo-Code for the Extend Path** :
```python
def extend_forward():
    # Stage 1: Attention for new tokens
    flash_attn_varlen_func()

    # Stage 2: Context Chunk Loop Processing
    for chunk in context_chunks: 
        cp_mha_gather_cache()
        flash_attn_varlen_func()
        merge_attn_states()

    # Stage 3: Get the final result
    merge_attn_states()
```

All those designs and implementation can unlock significant AMD GPU performance.

## Multi Latent Attention Backend

Having covered the four MHA backends, let's now explore Multi-Latent Attention (MLA) - an architectural innovation that dramatically reduces memory requirements through KV cache compression.

### TRITON MLA Backend

**Introduction to MLA in vLLM**

Multi-Latent Attention (MLA) is an architectural innovation introduced in DeepSeek-V2 and adopted by models like DeepSeek-V3 and Kimi. Unlike traditional Multi-Head Attention (MHA) which stores full key and value vectors for each token, MLA compresses the KV cache into a **latent representation** using learned low-rank projections.

The key insight: instead of storing full KV vectors (which might be 8192 dimensions for 128 heads √ó 64 dimensions), MLA stores a compressed latent vector of typically **576 dimensions** (composed of kv_lora_rank + qk_rope_dim, e.g., 512 + 64). This represents a **~14x memory reduction** for the KV cache, enabling much longer context lengths within the same memory budget.

**Design Overview**

The common.py file at `vllm/v1/attention/backends/mla/common.py` serves as the entry point for all hardware backends computing MLA operations. TRITON_MLA (`vllm/v1/attention/backends/mla/triton_mla.py`) implements the MLA backend using Triton kernels, providing a cross-platform solution that works on both NVIDIA and AMD hardware.

The backend employs a **two-path architecture**, choosing different computational strategies based on whether we're in decode or prefill mode:

- **Decode Path**: Data-movement optimized (memory-bound workload)
- **Prefill Path**: Compute optimized (compute-bound workload)

This dual-path design recognizes that decode and prefill have fundamentally different bottlenecks and optimizes each path accordingly.

**The Decode Path: Data-Movement Optimization**

During decode (single-token generation), the primary bottleneck is memory bandwidth‚Äîwe need to read the entire compressed KV cache to compute attention for just one new query token. To minimize data movement, TRITON_MLA uses a custom `decode_attention_fwd` Triton kernel that **fuses the latent KV expansion with attention computation**.

Without fusion, we would need to:
1. Read compressed KV from cache
2. Expand to full KV using learned projection matrices (expensive memory write)
3. Read the expanded KV back (expensive memory read)
4. Compute attention

With fusion, the kernel performs expansion on-the-fly within registers/shared memory, eliminating the expensive intermediate memory traffic. This makes decode operations primarily memory-bound on reading the compressed cache, rather than being bottlenecked by multiple passes over expanded data.

**Pseudo-code for Decode**:
```python
def triton_mla_decode():
    # Custom Triton kernel with fused expansion + attention
    output = decode_attention_fwd(
        query,                      # [batch, num_heads, head_dim]
        compressed_kv_cache,        # [blocks, block_size, 576] - Much smaller!
        kv_expansion_weights,       # Learned projection: 576 ‚Üí full dim
        block_tables,
        context_lens,
        ...
    )

    # Inside the kernel (conceptually):
    #   1. Read compressed KV chunk from cache
    #   2. Expand to full K, V on-the-fly in registers
    #   3. Compute attention scores and outputs
    #   4. Write final attention output
    # All done in one fused operation, minimizing memory traffic

    return output
```

**The Prefill Path: Compute Optimization**

During prefill, we process many tokens at once, making it a compute-bound operation (large Q @ K^T and attention @ V matrix multiplications). Here, the strategy differs: we're willing to accept some memory overhead if it leads to better compute efficiency.

For new tokens that haven't been compressed yet, we work with full-dimensional KV. For the existing context (already in compressed form in the cache), we process it in **chunks** to avoid OOM while maintaining high compute throughput.

**Enhanced Pseudo-code for Prefill**:
```python
def triton_mla_prefill():
    # Stage 1: Self-attention on new tokens
    # These tokens haven't been compressed yet, so we use full KV
    new_output, new_lse = _run_prefill_new_tokens(
        query_new,      # [new_tokens, num_heads, head_dim]
        key_new,        # Full dimensional, not compressed yet
        value_new,
        ...
    )

    # Initialize accumulators with new token results
    partial_outputs = [new_output]
    partial_lse = [new_lse]

    # Stage 2: Context chunk loop processing
    # Process existing compressed KV cache in chunks to bound memory
    CHUNK_SIZE = 8192  # Configurable based on available memory

    for chunk_start in range(0, context_len, CHUNK_SIZE):
        chunk_end = min(chunk_start + CHUNK_SIZE, context_len)

        # Gather and decompress this chunk of KV cache
        # Applies learned projection: compressed_kv @ W_UK ‚Üí key
        #                            compressed_kv @ W_UV ‚Üí value
        key_chunk, value_chunk = gather_and_maybe_dequant_cache(
            compressed_kv_cache,
            kv_expansion_weights,  # W_UK and W_UV projection matrices
            chunk_start,
            chunk_end,
            ...
        )

        # Compute cross-attention: new queries √ó cached context chunk
        chunk_output, chunk_lse = _run_prefill_context_chunk_fa(
            query_new,
            key_chunk,      # Expanded from compressed cache
            value_chunk,    # Expanded from compressed cache
            ...
        )

        partial_outputs.append(chunk_output)
        partial_lse.append(chunk_lse)

    # Stage 3: Merge all chunks using LSE for correct softmax
    final_output = merge_attn_states(partial_outputs, partial_lse)

    # Compress and cache the new KV for future decode steps
    # Apply learned compression: key @ W_DK ‚Üí compressed_kv
    #                           value @ W_DV ‚Üí (combined with above)
    compressed_kv = compress_kv(key_new, value_new, compression_weights)
    reshape_and_cache(compressed_kv, kv_cache, ...)

    return final_output
```

**When to Use TRITON_MLA**

TRITON_MLA is the right choice when:
- Using MLA-architecture models (DeepSeek-V2/V3, Kimi, etc.)
- Cross-platform deployments (works on both NVIDIA and AMD via Triton)
- You need a well-tested, community-maintained implementation

### ROCM_AITER_MLA Backend

**Design Overview**

ROCM_AITER_MLA (`vllm/v1/attention/backends/mla/rocm_aiter_mla.py`) is AMD's performance-optimized MLA implementation, specifically tuned for ROCm platforms and MI300 series GPUs. While TRITON_MLA provides cross-platform compatibility through Triton, ROCM_AITER_MLA leverages AMD-specific optimizations and assembly-level kernels from the external AITER library to deliver maximum performance on AMD hardware.

The backend employs two specialized kernels:
- **mla_decode_fwd**: Custom assembly-optimized decode kernel for AMD CDNA architecture
- **aiter.flash_attn_varlen_func**: Variable-length flash attention for prefill operations

Like TRITON_MLA, this backend uses the compressed KV cache layout `(num_blocks, block_size, 576)` where 576 is the latent dimension (kv_lora_rank + qk_rope_dim).

Beyond raw kernel performance, this backend inherits the full feature set of the `FlashMLABackend`, including `PIECEWISE_AND_FULL CUDA` graph support and MTP support. One other advantage is its near-identical performance across virtually any KV cache block size. In practice, this means you can treat every token as prefix cache without worrying about the performance penalties typically associated with fine-grained caching‚Äîenabling highly efficient long-context and multi-turn deployments.

**CUDA Graph Support**

ROCM_AITER_MLA implements comprehensive CUDA graph support with persistent buffer management. CUDA graphs reduce kernel launch overhead by capturing a sequence of GPU operations and replaying them as a single unit.

The backend supports two graph modes:
- **PIECEWISE mode**: Individual operations can be graphed separately
- **FULL_AND_PIECEWISE mode**: The decode path of MLA Attention Backend is captured in CUDA graph.

**Implementation Details**:
```python
# Persistent buffers allocated once and reused across graph replays
self.persistent_paged_kv_indptr = torch.empty(...)
self.persistent_paged_kv_indices = torch.empty(...)
self.persistent_qo_indptr = torch.empty(...)

# During graph replay, only tensor data updates, not allocations
# This eliminates memory allocation overhead from the critical path
```

**Performance Impact**:
- 10-15% latency reduction for decode in small batch scenarios
- More consistent latencies (reduced variance)
- Lower CPU overhead per request
- Essential for meeting strict latency SLAs in production


**Pseudo-code for Both Paths**

**Decode Path**:
```python
def rocm_aiter_mla_decode():
    # AMD-optimized assembly kernel with fused expansion + attention
    output = rocm_aiter_ops.mla_decode_fwd(
        query,                      # [batch, num_heads, head_dim]
        compressed_kv_cache,        # [blocks, block_size, 576]
        kv_expansion_weights,       # Learned projection matrices
        paged_kv_indptr,           # Efficient paged cache indexing
        paged_kv_indices,
        block_tables,
        block_size=1,
        ...
    )

    return output
```

**Prefill Path**:
```python
def rocm_aiter_mla_prefill():
    # Stage 1: New tokens with variable-length flash attention
    new_output, new_lse = aiter.flash_attn_varlen_func(
        query_new,
        key_new,        # Full dimensional
        value_new,
        cu_seqlens_q,   # Cumulative sequence lengths (variable-length support)
        cu_seqlens_k,
        ...
    )

    partial_outputs, partial_lse = [new_output], [new_lse]

    # Stage 2: Context chunks (similar to TRITON_MLA)
    for chunk_start in range(0, context_len, CHUNK_SIZE):
        chunk_end = min(chunk_start + CHUNK_SIZE, context_len)

        # Gather and decompress context chunk
        key_chunk, value_chunk = gather_and_maybe_dequant_cache(
            compressed_kv_cache,
            kv_expansion_weights,
            chunk_start,
            chunk_end,
            ...
        )

        # Cross-attention with variable-length support
        chunk_output, chunk_lse = aiter.flash_attn_varlen_func(
            query_new,
            key_chunk,
            value_chunk,
            cu_seqlens_q,
            cu_seqlens_k,
            ...
        )

        partial_outputs.append(chunk_output)
        partial_lse.append(chunk_lse)

    # Stage 3: Merge all chunks using LSE
    final_output = merge_attn_states(partial_outputs, partial_lse)

    # Compress and cache new KV
    compressed_kv = compress_kv(key_new, value_new, compression_weights)
    reshape_and_cache(compressed_kv, kv_cache, ...)

    return final_output
```

**Comparison with TRITON_MLA**

| Feature | TRITON_MLA | ROCM_AITER_MLA |
|---------|------------|----------------|
| **Decode Kernel** | Triton custom kernel | Assembly-optimized `mla_decode_fwd` |
| **Prefill Kernel** | Triton flash attention | Variable-length `aiter.flash_attn_varlen_func` |
| **CUDA Graph** | Yes | Yes |
| **Platform** | Cross-platform (Triton) | AMD ROCm specific |
| **Performance** | Good on NVIDIA & AMD | **Optimized for MI300/MI355** |
| **Maintenance** | Community | AMD + Community |


#### ROCM_AITER_TRITON_MLA Variant

ROCM_AITER_TRITON_MLA (`vllm/v1/attention/backends/mla/aiter_triton_mla.py`) is a minor variant of ROCM_AITER_MLA that differs only in its choice of flash attention kernel for prefill operations. While ROCM_AITER_MLA uses the standard `aiter.flash_attn_varlen_func`, ROCM_AITER_TRITON_MLA uses the Triton MHA implementation: `aiter.ops.triton.mha.flash_attn_varlen_func`.

**Key Difference**

The variant inherits all functionality from `AiterMLAImpl` but overrides the flash attention kernel selection. The only notable implementation difference is an **LSE transpose** operation to maintain compatibility with the Triton MHA kernel's output format:

```python
# ROCM_AITER_TRITON_MLA transposes LSE for compatibility
result = self.flash_attn_varlen_func(q, k, v, ...)

if type(result) is tuple and return_softmax_lse:
    output, lse = result
    lse = lse.T.contiguous()  # (q_len, num_heads) ‚Üí (num_heads, q_len)
    return (output, lse)
```

**Important**: Both variants share the **identical decode kernel** (`mla_decode_fwd`), all assembly-level optimizations, fine-grained prefix caching support, and CUDA graph capabilities. The only difference is the flash attention kernel used during prefill.


## Performance Benchmark

### MHA attention performance comparisions
### MLA attention performance comparision



## Get Started


